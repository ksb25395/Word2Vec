{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # We get tokens with atleast one alphabet using \n",
    "    # this regular expression\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "        \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i in range(N):\n",
    "        indices = list(range(max(0, i - window_size), i)) + \\\n",
    "                    list(range(i+1, min(N, i + window_size + 1)))\n",
    "        for j in indices:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial document\n",
    "doc = \"After the deduction of the costs of investing, \"\\\n",
    "        \"beating the stock market is a loser's game.\"\n",
    "# list of words in doc\n",
    "tokens = tokenize(doc)\n",
    "# word to id and id to word representation of each word and id\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "# generation of all pairs of (x, y) where x is the id of a word in doc\n",
    "# and y is the id of a word within specified window size in the doc\n",
    "X, Y = generate_training_data(tokens, word_to_id, window_size=3)\n",
    "# size of the vocabulary\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  5,  5, 12, 12, 12, 12,  7,  7,  7,  7,  7,  2,  2,  2,  2,\n",
       "         2,  2, 12, 12, 12, 12, 12, 12,  0,  0,  0,  0,  0,  0,  2,  2,\n",
       "         2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  8,  8,  8,  8,  8,  8,\n",
       "        12, 12, 12, 12, 12, 12,  3,  3,  3,  3,  3,  3,  6,  6,  6,  6,\n",
       "         6,  6, 11, 11, 11, 11, 11, 11,  4,  4,  4,  4,  4, 10, 10, 10,\n",
       "        10,  9,  9,  9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure: \n",
    "# 1) Initialize weights\n",
    "# 2) Forward prop\n",
    "# 3) Calculation of cost function\n",
    "# 4) Backprop\n",
    "# 5) Update weights\n",
    "# Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of parameters to train\n",
    "# 1) Word embedding layer \n",
    "# 2) Dense softmax layer\n",
    "# Shape of word embedding layer will be (vocab_size, emb_size)\n",
    "# Each row represents a word in one embedding of size emb_size\n",
    "# Shape of dense layer is (vocab_size, emb_size)\n",
    "# I/P to dense layer : (emb_size, #training_instances)\n",
    "# O/P of dense layer: (vocab_size, #training_instances)\n",
    "def initialize_word_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus training data\n",
    "    emb_size: int. word embedding size. How many dimensions the vocab is represented in\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size)*0.01\n",
    "    \n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: size of input in dense layer\n",
    "    output_size: size of output in dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size)\n",
    "    \n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    Initialize all training parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_word_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for forward propagation\n",
    "\n",
    "def ind_to_word_vecs(indices, parameters):\n",
    "    \"\"\"\n",
    "    indices: numpy array of ids of words (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \n",
    "    word_vec: We pick out all rows of WRD_EMB corresponding to \n",
    "    array indices and transpose the matrix. In resulting matrix, \n",
    "    each column corresponds to a word embedding of emb_size. \n",
    "    Matrix size is (emb_size, m)\n",
    "    \"\"\"\n",
    "    m = indices.shape[1]\n",
    "    WRD_EMB = parameters[\"WRD_EMB\"]\n",
    "    word_vec = WRD_EMB[indices.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array returned by ind_to_word_vecs. (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    \n",
    "    W = parameters['W']\n",
    "    m = word_vec.shape[1]\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z - output of the dense layer, shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis= 0, keepdims = True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "    \n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(indices, parameters):\n",
    "    word_vec = ind_to_word_vecs(indices, parameters)\n",
    "    W, Z = parameters['W'], linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['indices'] = indices\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of Cross-entropy cost\n",
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output of softmax\n",
    "    Y: actual labels\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = (-1/m)*np.sum(np.sum(Y*np.log(softmax_out+0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for backward propagation\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. result from each step of forward prop\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1/m)*np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    indices = caches['indices']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = indices.shape[-1]\n",
    "    \n",
    "    WRD_EMB[indices.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "    parameters['W'] -= learning_rate*gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training the model\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, \\\n",
    "                           epochs, batch_size=256, parameters=None, \\\n",
    "                           print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encoding of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent \n",
    "              each vocabulary\n",
    "    learning_rate: alpha in the weights update formula after backprop\n",
    "    epochs: how many epochs to train out model\n",
    "    batch_size: size of mini-batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print cost during training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_indices = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_indices)\n",
    "        for i in batch_indices:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "            \n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (50) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (50) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5489816728310304\n",
      "Cost after epoch 50: 5.128191262648\n",
      "Cost after epoch 100: 5.243063468087623\n",
      "Cost after epoch 150: 5.2534251318968455\n",
      "Cost after epoch 200: 5.218283881677052\n",
      "Cost after epoch 250: 5.143409026200252\n",
      "Cost after epoch 300: 5.020222183523817\n",
      "Cost after epoch 350: 4.838006785417612\n",
      "Cost after epoch 400: 4.6290830882634895\n",
      "Cost after epoch 450: 4.461360361379669\n",
      "Cost after epoch 500: 4.301220781284504\n",
      "Cost after epoch 550: 4.119348527030824\n",
      "Cost after epoch 600: 3.936220220058227\n",
      "Cost after epoch 650: 3.8001224689003994\n",
      "Cost after epoch 700: 3.689147511116835\n",
      "Cost after epoch 750: 3.5785342783155047\n",
      "Cost after epoch 800: 3.4709826297970023\n",
      "Cost after epoch 850: 3.3803795536418604\n",
      "Cost after epoch 900: 3.309240883815884\n",
      "Cost after epoch 950: 3.2480023473737147\n",
      "Cost after epoch 1000: 3.191897455905921\n",
      "Cost after epoch 1050: 3.1397138084014458\n",
      "Cost after epoch 1100: 3.0914598363087156\n",
      "Cost after epoch 1150: 3.0480989945036243\n",
      "Cost after epoch 1200: 3.009773019227393\n",
      "Cost after epoch 1250: 2.9753027636173415\n",
      "Cost after epoch 1300: 2.944273675281354\n",
      "Cost after epoch 1350: 2.917816833779091\n",
      "Cost after epoch 1400: 2.8968145972613217\n",
      "Cost after epoch 1450: 2.88003471145666\n",
      "Cost after epoch 1500: 2.8648256002950094\n",
      "Cost after epoch 1550: 2.8501692462826766\n",
      "Cost after epoch 1600: 2.8359661357390964\n",
      "Cost after epoch 1650: 2.8214883853408206\n",
      "Cost after epoch 1700: 2.8058459171814363\n",
      "Cost after epoch 1750: 2.7879710631370296\n",
      "Cost after epoch 1800: 2.76674687733975\n",
      "Cost after epoch 1850: 2.7418804231274496\n",
      "Cost after epoch 1900: 2.7143634673224595\n",
      "Cost after epoch 1950: 2.6855203670203887\n",
      "Cost after epoch 2000: 2.655912775510605\n",
      "Cost after epoch 2050: 2.6253226820110855\n",
      "Cost after epoch 2100: 2.593495146645386\n",
      "Cost after epoch 2150: 2.5606527764503144\n",
      "Cost after epoch 2200: 2.527673820006308\n",
      "Cost after epoch 2250: 2.495904190394236\n",
      "Cost after epoch 2300: 2.4664630136784527\n",
      "Cost after epoch 2350: 2.4397263300099077\n",
      "Cost after epoch 2400: 2.415536921507782\n",
      "Cost after epoch 2450: 2.393685943228163\n",
      "Cost after epoch 2500: 2.3741433621793906\n",
      "Cost after epoch 2550: 2.3569887428866423\n",
      "Cost after epoch 2600: 2.3422466498103818\n",
      "Cost after epoch 2650: 2.329803476184469\n",
      "Cost after epoch 2700: 2.3194085247323275\n",
      "Cost after epoch 2750: 2.3107240989038917\n",
      "Cost after epoch 2800: 2.3034167698642167\n",
      "Cost after epoch 2850: 2.2972240456130493\n",
      "Cost after epoch 2900: 2.291957379009061\n",
      "Cost after epoch 2950: 2.2874681165581805\n",
      "Cost after epoch 3000: 2.283604341996372\n",
      "Cost after epoch 3050: 2.280161033930267\n",
      "Cost after epoch 3100: 2.276817318988371\n",
      "Cost after epoch 3150: 2.273089489070811\n",
      "Cost after epoch 3200: 2.2684249063035633\n",
      "Cost after epoch 3250: 2.2625643200279266\n",
      "Cost after epoch 3300: 2.255899139052694\n",
      "Cost after epoch 3350: 2.2493148754980625\n",
      "Cost after epoch 3400: 2.2436345366930603\n",
      "Cost after epoch 3450: 2.2391759356785634\n",
      "Cost after epoch 3500: 2.235755981110948\n",
      "Cost after epoch 3550: 2.233013169964849\n",
      "Cost after epoch 3600: 2.230646698701759\n",
      "Cost after epoch 3650: 2.2284669945536226\n",
      "Cost after epoch 3700: 2.2263684556343213\n",
      "Cost after epoch 3750: 2.22429761719567\n",
      "Cost after epoch 3800: 2.2222324901660673\n",
      "Cost after epoch 3850: 2.2201705633405777\n",
      "Cost after epoch 3900: 2.21812147749894\n",
      "Cost after epoch 3950: 2.216101978485852\n",
      "Cost after epoch 4000: 2.2141320522836163\n",
      "Cost after epoch 4050: 2.2122318476689053\n",
      "Cost after epoch 4100: 2.2104193120140194\n",
      "Cost after epoch 4150: 2.2087085731951643\n",
      "Cost after epoch 4200: 2.2071090889273797\n",
      "Cost after epoch 4250: 2.2056255116974346\n",
      "Cost after epoch 4300: 2.204258130317095\n",
      "Cost after epoch 4350: 2.2030036874454733\n",
      "Cost after epoch 4400: 2.2018563581525314\n",
      "Cost after epoch 4450: 2.2008087066883295\n",
      "Cost after epoch 4500: 2.1998524988048973\n",
      "Cost after epoch 4550: 2.1989793121893975\n",
      "Cost after epoch 4600: 2.198180940635456\n",
      "Cost after epoch 4650: 2.1974496212390777\n",
      "Cost after epoch 4700: 2.196778129022047\n",
      "Cost after epoch 4750: 2.1961597849154155\n",
      "Cost after epoch 4800: 2.195588416601021\n",
      "Cost after epoch 4850: 2.195058301822863\n",
      "Cost after epoch 4900: 2.194564113434796\n",
      "Cost after epoch 4950: 2.1941008763731547\n",
      "Cost after epoch 5000: 2.1936639397783892\n",
      "Cost after epoch 5050: 2.193248962887446\n",
      "Cost after epoch 5100: 2.192851910906634\n",
      "Cost after epoch 5150: 2.192469056400419\n",
      "Cost after epoch 5200: 2.192096982208745\n",
      "Cost after epoch 5250: 2.191732582960945\n",
      "Cost after epoch 5300: 2.1913730634368904\n",
      "Cost after epoch 5350: 2.191015933054422\n",
      "Cost after epoch 5400: 2.190658996513291\n",
      "Cost after epoch 5450: 2.190300341089881\n",
      "Cost after epoch 5500: 2.1899383213043713\n",
      "Cost after epoch 5550: 2.1895715417423123\n",
      "Cost after epoch 5600: 2.1891988387698946\n",
      "Cost after epoch 5650: 2.1888192617854543\n",
      "Cost after epoch 5700: 2.188432054531542\n",
      "Cost after epoch 5750: 2.1880366368720647\n",
      "Cost after epoch 5800: 2.1876325873283227\n",
      "Cost after epoch 5850: 2.1872196265713386\n",
      "Cost after epoch 5900: 2.1867976019876445\n",
      "Cost after epoch 5950: 2.1863664733715638\n",
      "Cost after epoch 6000: 2.185926299747914\n",
      "Cost after epoch 6050: 2.185477227293351\n",
      "Cost after epoch 6100: 2.185019478300336\n",
      "Cost after epoch 6150: 2.1845533411129705\n",
      "Cost after epoch 6200: 2.1840791609567463\n",
      "Cost after epoch 6250: 2.183597331582877\n",
      "Cost after epoch 6300: 2.1831082876506325\n",
      "Cost after epoch 6350: 2.1826124977767503\n",
      "Cost after epoch 6400: 2.182110458188318\n",
      "Cost after epoch 6450: 2.181602686923662\n",
      "Cost after epoch 6500: 2.181089718534022\n",
      "Cost after epoch 6550: 2.180572099246616\n",
      "Cost after epoch 6600: 2.1800503825567232\n",
      "Cost after epoch 6650: 2.1795251252223964\n",
      "Cost after epoch 6700: 2.178996883640259\n",
      "Cost after epoch 6750: 2.1784662105844297\n",
      "Cost after epoch 6800: 2.1779336522930786\n",
      "Cost after epoch 6850: 2.1773997458884478\n",
      "Cost after epoch 6900: 2.176865017116585\n",
      "Cost after epoch 6950: 2.1763299783926366\n",
      "Cost after epoch 7000: 2.1757951271365865\n",
      "Cost after epoch 7050: 2.1752609443829547\n",
      "Cost after epoch 7100: 2.17472789364644\n",
      "Cost after epoch 7150: 2.1741964200239465\n",
      "Cost after epoch 7200: 2.173666949512134\n",
      "Cost after epoch 7250: 2.173139888518616\n",
      "Cost after epoch 7300: 2.1726156235444716\n",
      "Cost after epoch 7350: 2.172094521015753\n",
      "Cost after epoch 7400: 2.171576927242329\n",
      "Cost after epoch 7450: 2.171063168483585\n",
      "Cost after epoch 7500: 2.170553551102251\n",
      "Cost after epoch 7550: 2.1700483617897306\n",
      "Cost after epoch 7600: 2.169547867848743\n",
      "Cost after epoch 7650: 2.169052317521628\n",
      "Cost after epoch 7700: 2.1685619403552296\n",
      "Cost after epoch 7750: 2.168076947595634\n",
      "Cost after epoch 7800: 2.167597532608191\n",
      "Cost after epoch 7850: 2.1671238713199767\n",
      "Cost after epoch 7900: 2.1666561226832304\n",
      "Cost after epoch 7950: 2.166194429159198\n",
      "Cost after epoch 8000: 2.1657389172223103\n",
      "Cost after epoch 8050: 2.1652896978847824\n",
      "Cost after epoch 8100: 2.164846867241534\n",
      "Cost after epoch 8150: 2.1644105070349546\n",
      "Cost after epoch 8200: 2.1639806852384775\n",
      "Cost after epoch 8250: 2.1635574566573523\n",
      "Cost after epoch 8300: 2.163140863544378\n",
      "Cost after epoch 8350: 2.162730936227833\n",
      "Cost after epoch 8400: 2.1623276937483604\n",
      "Cost after epoch 8450: 2.1619311445012683\n",
      "Cost after epoch 8500: 2.1615412868804733\n",
      "Cost after epoch 8550: 2.161158109920288\n",
      "Cost after epoch 8600: 2.1607815939312673\n",
      "Cost after epoch 8650: 2.1604117111265513\n",
      "Cost after epoch 8700: 2.16004842623534\n",
      "Cost after epoch 8750: 2.159691697100515\n",
      "Cost after epoch 8800: 2.159341475257766\n",
      "Cost after epoch 8850: 2.1589977064939725\n",
      "Cost after epoch 8900: 2.158660331383034\n",
      "Cost after epoch 8950: 2.1583292857977177\n",
      "Cost after epoch 9000: 2.1580045013964915\n",
      "Cost after epoch 9050: 2.1576859060846756\n",
      "Cost after epoch 9100: 2.157373424449567\n",
      "Cost after epoch 9150: 2.1570669781695004\n",
      "Cost after epoch 9200: 2.156766486397057\n",
      "Cost after epoch 9250: 2.1564718661168425\n",
      "Cost after epoch 9300: 2.156183032478476\n",
      "Cost after epoch 9350: 2.1558998991055214\n",
      "Cost after epoch 9400: 2.1556223783812807\n",
      "Cost after epoch 9450: 2.155350381712387\n",
      "Cost after epoch 9500: 2.1550838197712494\n",
      "Cost after epoch 9550: 2.154822602718423\n",
      "Cost after epoch 9600: 2.1545666404059842\n",
      "Cost after epoch 9650: 2.1543158425630278\n",
      "Cost after epoch 9700: 2.1540701189643707\n",
      "Cost after epoch 9750: 2.15382937958353\n",
      "Cost after epoch 9800: 2.1535935347310224\n",
      "Cost after epoch 9850: 2.153362495179002\n",
      "Cost after epoch 9900: 2.153136172273186\n",
      "Cost after epoch 9950: 2.1529144780330274\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  4,  8, 12, 10,  0,  8,  8,  0,  8, 10, 10,  8],\n",
       "       [ 1,  8,  0,  6,  3,  7,  4, 12,  3,  4,  3,  3, 12],\n",
       "       [ 2,  3,  2, 11,  6, 12, 11,  5, 12, 11,  6,  6,  2],\n",
       "       [ 8, 12, 12,  4,  9,  2, 10,  0,  6, 10,  9,  9, 11]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_indices = np.argsort(softmax_test, axis=0)[-4:, :]\n",
    "top_sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costs's neighbor words: ['beating', 'of', 'investing', 'after']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', 'a']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "stock's neighbor words: ['a', 'is', 'market', 'the']\n",
      "a's neighbor words: ['game', 'market', 'stock', \"loser's\"]\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "market's neighbor words: [\"loser's\", 'is', 'a', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "beating's neighbor words: ['market', 'the', 'stock', 'costs']\n",
      "game's neighbor words: [\"loser's\", 'is', 'a', 'beating']\n",
      "loser's's neighbor words: ['game', 'market', 'stock', \"loser's\"]\n",
      "is's neighbor words: ['game', 'market', 'stock', \"loser's\"]\n",
      "the's neighbor words: ['is', 'of', 'the', 'beating']\n"
     ]
    }
   ],
   "source": [
    "for input_index in range(vocab_size):\n",
    "    output_word_indices = top_sorted_indices[::-1, input_index]\n",
    "    input_word = id_to_word[input_index]\n",
    "    output_words = [id_to_word[output_index] for output_index in \\\n",
    "                    output_word_indices]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
